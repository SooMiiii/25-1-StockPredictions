{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1차 목표\n",
        "- 인코더 구현\n",
        "\n",
        "\n",
        "2차 목표\n",
        "- add norm 빼먹은 거 넣기\n",
        "- 마스킹 넣기\n",
        "- 디코더 구현\n",
        "- 트랜스포머 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SruX-e0Jy0A4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "\n",
        "# import torchvision.datasets as datasets\n",
        "# import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGwnXyTByxGQ",
        "outputId": "d586bbce-395c-49a1-d43b-695d93ea4c3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# # 데이터\n",
        "# transform = transforms.ToTensor()\n",
        "\n",
        "# trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "# testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYKTISGAy8-W"
      },
      "source": [
        "인코더에 필요한 것\n",
        "- 포지셔널 인코딩\n",
        "- 멀티헤드 어텐션\n",
        "- Normalization & Add\n",
        "- Feed forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. PositionalEncoding\n",
        "- (기존 코드) pos와 i는 shape가 달라서 바로 연산 불가능 ->  각각 2차원으로 확장해야 함\n",
        "- (기존 코드) pos_emb = torch.zeros(seq_len, d_model, device=x.device) 에서 device를 명시해주지 않으면 오류가 발생할 수도\n",
        "\n",
        "### 코드 변경\n",
        "1. 위치 인코딩 미리 계산\n",
        "- 위치 인코딩 값을 __init__() 메서드에서 미리 계산하여 저장\n",
        "- 최대 길이(max_len)까지의 위치 인코딩을 계산하고, 이를 self.pe에 저장\n",
        "2. 버퍼로 등록\n",
        "- self.register_buffer('pe', pe)를 사용해 위치 인코딩 값을 모델의 버퍼로 등록\n",
        "- 이를 통해 위치 인코딩은 학습되지 않는 값으로 관리되며, 모델 저장/로드 및 디바이스 이동 시 자동으로 처리된다.\n",
        "3. 입력 시퀀스 길이에 맞게 필요한 부분(self.pe[:, :seq_len, :])만 잘라서 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ4g88Y2yyr0"
      },
      "outputs": [],
      "source": [
        "# class PositionalEncoding(nn.Module):\n",
        "#     def __init__(self, d_model):\n",
        "#         super().__init__()\n",
        "#         self.d_model = d_model\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: (batch_size, seq_len, d_model)\n",
        "#         _batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "#         # pos 위치\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1) # (seq_len,1)\n",
        "#         # i 차원\n",
        "#         i = torch.arange(d_model).unsqueeze(0) # (1,d_model)\n",
        "\n",
        "#         angle_rates = pos / (10000 ** (2 * (i // 2) / self.d_model))\n",
        "\n",
        "#         pos_emb = torch.zeros(seq_len, d_model, device=x.device)  # (seq_len, d_model)\n",
        "#         pos_emb[:, 0::2] = torch.sin(angle_rates[:, 0::2])  # 짝수 인덱스\n",
        "#         pos_emb[:, 1::2] = torch.cos(angle_rates[:, 1::2])  # 홀수 인덱스\n",
        "\n",
        "#         pos_emb = pos_emb.unsqueeze(0)  # (1, seq_len, d_model)\n",
        "#         return x + pos_emb\n",
        "    \n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # 위치 인코딩 미리 계산\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        # 버퍼로 등록 (모델 파라미터로 취급되지 않음)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        _batch_size, seq_len, _ = x.shape\n",
        "        return x + self.pe[:, :seq_len, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Masking\n",
        "- `torch.tril`: 하삼각 행렬 생성\n",
        "- `seq_len`: 입력 시퀀스의 길이, 현재 시점 이후의 값을 False로 설정\n",
        "- `unsqueeze`를 두 번 호출하여 마스크의 차원을 확장\n",
        "    - 최종 shape: `(1, 1, seq_len, seq_len)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Masking(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (batch_size, seq_len, data_dim)\n",
        "        _, seq_len, _ = x.shape\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=x.device))\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ_pzwmb88MP"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        \n",
        "        # d_k와 d_v는 d_model을 num_heads로 나눈 값으로 설정\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # 헤드당 차원\n",
        "        self.d_v = d_model // num_heads  # 헤드당 차원\n",
        "        \n",
        "        # 선형 투영을 위한 가중치 행렬\n",
        "        self.W_q = nn.Linear(d_model, d_model)  # (d_model -> d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)  # (d_model -> d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)  # (d_model -> d_model)\n",
        "        \n",
        "        # 출력 투영을 위한 가중치 행렬\n",
        "        self.W_o = nn.Linear(d_model, d_model)  # (d_model -> d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)  # 입력의 배치 크기\n",
        "\n",
        "        # 선형 투영: (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
        "        Q = self.W_q(query)  # Query 변환\n",
        "        K = self.W_k(key)    # Key 변환\n",
        "        V = self.W_v(value)  # Value 변환\n",
        "\n",
        "        # 헤드로 분할: (batch_size, seq_len, d_model) -> (batch_size, num_heads, seq_len, d_k)\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
        "\n",
        "        # 스케일드 닷-프로덕트 어텐션 계산:\n",
        "        # Q와 K의 내적을 계산하여 어텐션 스코어를 얻음\n",
        "        # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # 마스킹 적용: 특정 위치를 무시하기 위해 큰 음수 값(-1e9)을 더함\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # 어텐션 가중치 계산: 소프트맥스를 통해 확률 분포로 변환\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # 어텐션 출력 계산: 가중치를 Value에 곱함\n",
        "        # (batch_size, num_heads, seq_len_q, d_v)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "\n",
        "        # 헤드 연결: (batch_size, num_heads, seq_len_q, d_v) -> (batch_size, seq_len_q/d_k)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "            # contiguous()는 비연속적인 텐서를 연속적인 메모리 배치로 변환, view를 쓸 때 같이 씀\n",
        "\n",
        "        # 최종 선형 투영: (batch_size, seq_len_q/d_k) -> (batch_size/seq_len/d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeyU1cqOG68h"
      },
      "source": [
        "## 4. FFN\n",
        "- FFN(x)=ReLU(xW1 +b1)W2 +b\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chGW9LyNFwGN"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMFXqGpaHV87"
      },
      "source": [
        "## 5. Norm\n",
        "- LayerNorm(x+sublayer(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMceTDodF0LH"
      },
      "outputs": [],
      "source": [
        "class ResidualLayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)  # Layer Normalization\n",
        "        self.dropout = nn.Dropout(dropout)      # Dropout\n",
        "\n",
        "    def forward(self, x, sublayer_output):\n",
        "        # Residual Connection + Layer Normalization\n",
        "        return self.layer_norm(x + self.dropout(sublayer_output))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. EncoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Tb9qVNFUHk"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8, ff_dim=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "        # Feed Forward Network (FFN)\n",
        "        self.ffn = FeedForwardNetwork(d_model=d_model, ff_dim=ff_dim, dropout=dropout)\n",
        "\n",
        "        # Residual Connection + Layer Normalization\n",
        "        self.norm1 = ResidualLayerNorm(d_model=d_model, dropout=dropout)\n",
        "        self.norm2 = ResidualLayerNorm(d_model=d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        # Multi-Head Attention + Residual Connection + LayerNorm\n",
        "        attn_output = self.mha(x, x, x, mask=src_mask)  # Self-attention: query=key=value=x\n",
        "        out1 = self.norm1(x, attn_output)\n",
        "\n",
        "        # Feed Forward Network + Residual Connection + LayerNorm\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.norm2(out1, ffn_output)\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. DecoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8, ff_dim=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Self Attention\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        \n",
        "        # Encoder-Decoder Attention\n",
        "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        \n",
        "        # Feed Forward Network\n",
        "        self.ffn = FeedForwardNetwork(d_model=d_model, ff_dim=ff_dim, dropout=dropout)\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm1 = ResidualLayerNorm(d_model=d_model, dropout=dropout)\n",
        "        self.norm2 = ResidualLayerNorm(d_model=d_model, dropout=dropout)\n",
        "        self.norm3 = ResidualLayerNorm(d_model=d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, dec_input, enc_output, dec_mask=None):\n",
        "        # Self-Attention + Residual Connection + LayerNorm\n",
        "        self_attn_output = self.self_attention(dec_input, dec_input, dec_input, mask=dec_mask)\n",
        "        out1 = self.norm1(dec_input, self_attn_output)\n",
        "\n",
        "        # Encoder-Decoder Attention + Residual Connection + LayerNorm\n",
        "        enc_dec_attn_output = self.enc_dec_attention(out1, enc_output, enc_output)\n",
        "        out2 = self.norm2(out1, enc_dec_attn_output)\n",
        "\n",
        "        # Feed Forward Network + Residual Connection + LayerNorm\n",
        "        ffn_output = self.ffn(out2)\n",
        "        out3 = self.norm3(out2, ffn_output)\n",
        "\n",
        "        return out3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim=3, num_layers=3, max_len=512, d_model=16, num_heads=4, ff_dim=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # 입력 차원을 모델 차원으로 변환\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "        \n",
        "        self.positional_encoding = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(\n",
        "                d_model=d_model,\n",
        "                num_heads=num_heads,\n",
        "                ff_dim=ff_dim,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x = self.positional_encoding(x)\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "            \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim=3, num_layers=3, max_len=512, d_model=16, num_heads=4, ff_dim=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(\n",
        "                d_model=d_model,\n",
        "                num_heads=num_heads,\n",
        "                ff_dim=ff_dim,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, enc_output, dec_input, dec_mask=None):\n",
        "        y = self.embedding(dec_input)\n",
        "\n",
        "        y = self.positional_encoding(y)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            y = layer(dec_input=y, enc_output=enc_output, dec_mask=dec_mask)\n",
        "            \n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_dim=3, output_dim=3, num_layers=3, max_len=512, \n",
        "                 d_model=16, num_heads=4, ff_dim=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 인코더\n",
        "        self.encoder = Encoder(\n",
        "            input_dim=input_dim,\n",
        "            num_layers=num_layers,\n",
        "            max_len=max_len,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # 디코더\n",
        "        self.decoder = Decoder(\n",
        "            input_dim=input_dim,\n",
        "            num_layers=num_layers,\n",
        "            max_len=max_len,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # 마스킹 레이어\n",
        "        self.masking = Masking()\n",
        "        \n",
        "        # 출력 레이어\n",
        "        self.output_layer = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # 마스크 생성 (디코더용)\n",
        "        dec_mask = self.masking(tgt)\n",
        "\n",
        "        # 인코더 통과\n",
        "        enc_output = self.encoder(src)\n",
        "\n",
        "        # 디코더 통과\n",
        "        dec_output = self.decoder(enc_output, tgt, dec_mask)\n",
        "\n",
        "        # 출력 레이어 통과\n",
        "        output = self.output_layer(dec_output)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
