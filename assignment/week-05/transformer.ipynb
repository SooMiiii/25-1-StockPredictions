{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1차 목표\n",
        "- 인코더 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SruX-e0Jy0A4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# import torchvision.datasets as datasets\n",
        "# import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGwnXyTByxGQ",
        "outputId": "d586bbce-395c-49a1-d43b-695d93ea4c3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# # 데이터\n",
        "# transform = transforms.ToTensor()\n",
        "\n",
        "# trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "# testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYKTISGAy8-W"
      },
      "source": [
        "인코더에 필요한 것\n",
        "- 포지셔널 인코딩\n",
        "- 멀티헤드 어텐션\n",
        "- Normalization & Add\n",
        "- Feed forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. PositionalEncoding\n",
        "- pos와 i는 shape가 달라서 바로 연산 불가능 ->  각각 2차원으로 확장해야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LJ4g88Y2yyr0"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # pos 위치\n",
        "        pos = torch.arange(seq_len).unsqueeze(1) # (seq_len,1)\n",
        "        # i 차원\n",
        "        i = torch.arange(d_model).unsqueeze(0) # (1,d_model)\n",
        "\n",
        "        angle_rates = pos / (10000 ** (2 * (i // 2) / self.d_model))\n",
        "\n",
        "        pos_emb = torch.zeros(seq_len, d_model)\n",
        "        pos_emb[:, 0::2] = torch.sin(angle_rates[:, 0::2])  # 짝수 인덱스\n",
        "        pos_emb[:, 1::2] = torch.cos(angle_rates[:, 1::2])  # 홀수 인덱스\n",
        "\n",
        "        pos_emb = pos_emb.unsqueeze(0)  # (1, seq_len, d_model)\n",
        "        return x + pos_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FW0olP21YF1",
        "outputId": "1d28cb24-3614-435e-f2e0-58801826c2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 10, 512])\n",
            "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "# 테스트\n",
        "x = torch.zeros(2, 10, 512)  # (batch_size=2, seq_len=10, d_model=512)\n",
        "pe = PositionalEncoding(d_model=512)\n",
        "\n",
        "out = pe(x)\n",
        "print(out.shape)  # (2, 10, 512) 나와야\n",
        "print(out[0, 0, :10])  # 첫 번째 토큰의 첫 10개 값 찍어보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DZ_pzwmb88MP"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_k)\n",
        "        self.W_k = nn.Linear(d_model, d_k)\n",
        "        self.W_v = nn.Linear(d_model, d_v)\n",
        "\n",
        "        self.W_o = nn.Linear(d_v, d_model)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # 입력 query.shape = (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, seq_len, d_k)\n",
        "        Q = self.W_q(query)\n",
        "        K = self.W_k(key)\n",
        "        V = self.W_v(value)\n",
        "\n",
        "        # 멀티헤드라서...\n",
        "        # (batch_size, seq_len, num_heads, d_k_head)\n",
        "            # transpose(1 ,2)를 사용해 head 축을 앞으로 -> 병렬 처리를 위함(이래....응....)\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k // self.num_heads).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_k // self.num_heads).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_v // self.num_heads).transpose(1, 2)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len, seq_len)\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k // self.num_heads) ** 0.5\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len, d_v_head)\n",
        "        attn_output = torch.matmul(attn_probs, V)\n",
        "\n",
        "        # (batch_size, seq_len, num_heads, d_v_head)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "            # contiguous()는 비연속적인 텐서를 연속적인 메모리 배치로 변환, view를 쓸 때 같이 씀\n",
        "\n",
        "        # (batch_size, seq_len, d_v)\n",
        "        attn_output = attn_output.view(batch_size, -1, self.num_heads * (self.d_v // self.num_heads))\n",
        "\n",
        "        # (batch_size, seq_len, d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB51PNHeBUMH",
        "outputId": "e6065e80-7bed-41fe-d80c-ad35e0bbf8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ MultiHeadAttention 통과!\n"
          ]
        }
      ],
      "source": [
        "# 테스트\n",
        "# 하이퍼파라미터\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "d_model = 16\n",
        "d_k = 16\n",
        "d_v = 16\n",
        "num_heads = 4\n",
        "\n",
        "# 모델 생성\n",
        "mha = MultiHeadAttention(d_model, d_k, d_v, num_heads)\n",
        "\n",
        "# 가짜 입력 데이터 생성\n",
        "query = torch.randn(batch_size, seq_len, d_model)\n",
        "key = torch.randn(batch_size, seq_len, d_model)\n",
        "value = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# 모델 실행\n",
        "output = mha(query, key, value)\n",
        "\n",
        "# 출력 결과 확인\n",
        "assert output.shape == (batch_size, seq_len, d_model)\n",
        "print(\"✅ MultiHeadAttention 통과!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeyU1cqOG68h"
      },
      "source": [
        "## 3. FFN\n",
        "- FFN(x)=ReLU(xW1 +b1)W2 +b\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "chGW9LyNFwGN"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMFXqGpaHV87"
      },
      "source": [
        "## 4. Norm\n",
        "- LayerNorm(x+sublayer(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oMceTDodF0LH"
      },
      "outputs": [],
      "source": [
        "class ResidualLayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)  # Layer Normalization\n",
        "        self.dropout = nn.Dropout(dropout)      # Dropout\n",
        "\n",
        "    def forward(self, x, sublayer_output):\n",
        "        # Residual Connection + Layer Normalization\n",
        "        return self.layer_norm(x + self.dropout(sublayer_output))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. EncoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Tb9qVNFUHk"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8, ff_dim=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head attention\n",
        "        d_k = d_v = d_model  # Query, Key, Value의 차원은 d_model로 설정 \n",
        "        self.mha = MultiHeadAttention(d_model=d_model,\n",
        "                                      d_k=d_k,\n",
        "                                      d_v=d_v,\n",
        "                                      num_heads=num_heads)\n",
        "\n",
        "        # Feed Forward Network (FFN)\n",
        "        self.ffn = FeedForwardNetwork(d_model=d_model,\n",
        "                                      ff_dim=ff_dim,\n",
        "                                      dropout=dropout)\n",
        "\n",
        "        # Residual Connection + Layer Normalization\n",
        "        self.norm1 = ResidualLayerNorm(d_model=d_model, dropout=dropout)\n",
        "        self.norm2 = ResidualLayerNorm(d_model=d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-Head Attention + Residual Connection + LayerNorm\n",
        "        attn_output = self.mha(x, x, x)  # Self-attention: query=key=value=x\n",
        "        out1 = self.norm1(x, attn_output)\n",
        "\n",
        "        # Feed Forward Network + Residual Connection + LayerNorm\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.norm2(out1, ffn_output)\n",
        "\n",
        "        return out2\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
